\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{mathptmx}

\topmargin 0.0in
\setlength{\textwidth} {420pt}
\setlength{\textheight} {620pt} 
\setlength{\oddsidemargin} {20pt}
\setlength{\marginparwidth} {72in}

\usepackage{fancyhdr} 
\usepackage{url}

% set it so that subsubsections have numbers and they
% are displayed in the TOC (maybe hard to read, might want to disable)

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

% define widow protection

\def\widow#1{\vskip #1\vbadness10000\penalty-200\vskip-#1}

\clubpenalty=10000  % Don't allow orphans
\widowpenalty=10000 % Don't allow widows

% this should give me the ability to use some math symbols that 
% were available by default in standard latex (i.e. \Box)

\usepackage{latexsym}

% define a little section heading that doesn't go with any number

\def\littlesection#1{
\widow{2cm}
\vskip 0.5cm
\noindent{\bf #1}
\vskip 0.0001cm 
}

\pagestyle{fancyplain}

\newcommand{\tstamp}{\today}   
\renewcommand{\sectionmark}[1]{\markright{#1}}
\lhead[\Section \thesection]            {\fancyplain{}{\rightmark}}
\chead[\fancyplain{}{}]                 {\fancyplain{}{}}
\rhead[\fancyplain{}{\rightmark}]       {\fancyplain{}{\thepage}}
\cfoot[\fancyplain{\thepage}{}]         {\fancyplain{\thepage}{}}

\newlength{\myVSpace}% the height of the box
\setlength{\myVSpace}{1ex}% the default, 
\newcommand\xstrut{\raisebox{-.5\myVSpace}% symmetric behaviour, 
  {\rule{0pt}{\myVSpace}}%
}

% leave things with no spacing extra spacing in the final version of the paper
\renewcommand{\baselinestretch}{1.0}    % must go before the begin of doc

% suppress the use of indentation for a paragraph

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}
\setlength{\headheight}{15pt}

\begin{document}

% handle widows appropriately
\def\widow#1{\vskip #1\vbadness10000\penalty-200\vskip-#1}

% build the title section

\makeatletter

\def\maketitle{%
  %\null
  \thispagestyle{empty}%
  %\vfill
  \begin{center}%\leavevmode
    %\normalfont
    {\Huge \@title\par}%
    %\hrulefill\par
    {\normalsize \@author\par}%
    \vskip .4in
%    {\Large \@date\par}%
  \end{center}%
  %\vfill
  %\null
  %\cleardoublepage

  }

\makeatother

\vspace*{-1.5in} \title{Paper Review:\\{\Large Search-Based Testing of Relational Schema Integrity Constraints Across Multiple Database Management Systems}}

% build the author section
\author{\vspace*{.1in} Braden D. Licastro\\
Department of Computer Science\\
Allegheny College \\
{\tt licastb@allegheny.edu}  \\
\url{http://www.fullforceapps.com/} \\ 
\vspace*{.1in} \today \\ 
}

% use the default title stuff
\maketitle

\vspace*{-.8in}
\section{Summary}
\label{sec:summary}
\vspace*{-.1in}

This paper describes an effective and efficient method of testing relational database schema for validity. sorting algorithm coined Triton Sort. The authors created a system, coined SchemaAnalyst, which operates at competitive or better generation times which outperform a commonly used, free, and open source system called DBMonster in terms of constraint coverage and mutation score. \cite{Kapfhammer2013} Often times when creating a relational schema mistakes can go unnoticed for varying lengths of time that can lead to data integrity problems and corruption down the line. The program and methods proposed combat this problem my verifying and testing the given schema constraints.

SchemaAnalyst was designed to create randomized data to be inserted into the database that would both violate the schema and also satisfy it. To successfully test this, data would be generated that should satisfy the constraints and be inserted into the empty database. If one of the correct inserts would not be inserted there is more than likely a constraint error in the database. The next pass discussed breaks the constraints down piece by piece. Through each iteration of insert statement the data generated will satisfy all schema requirements but one constraint. This could be a unique, not null, primary key, and so on. By violating one constraint at a time each constraint in the schema may be validated.

Lastly, a set of mutation operators was used to test the integrity constraints of the schema. \cite{Kapfhammer2013} This segment of the research mutated the constraints one part at a time. Examples could include removing a NOT NULL constraint or adding one where there previously wasn't. This will test the schema in a way that would verify that only correct data is allowable therefore verifying the schema. Another method the authors implemented was the removal, addition, or replacement of a primary or foreign key constraint.

\vspace*{-.1in}
\section{Critique}
\label{sec:critique}
\vspace*{-.1in}

Throughout the paper, the authors restricted the validations to the universal data types, namely \em{Boolean, DateTime, Date, Numeric, String, Time} and \em{TimeStamp} \cite{Kapfhammer2013} due to the variance of allowable data types in various DBMS implementations. In the event where a data type is used that cannot be mapped to one of these universal types, one could assume validation would not be possible. I believe that by focusing on one DBMS which both SchemaAnalyst and DBMonster are compatible with, a more in-depth validation program would be possible. In addition, the generation of data is done based on the provided schema and is therefore tested based on the schema provided. Could it be possible that a specific type or severity would alter the generated data so severely that the expected data appears nothing like the generated data? In this case it could be theoretically possible to find minor constraint errors but test using invalid data.

\vspace*{-.1in}
\section{Synthesis}
\label{sec:synthesis}
\vspace*{-.1in}

After reading the research paper I concluded that additional research could be performed in several ways, primarily creating a testing environment that would be fully automated and determine why a certain mutant wasn't fixed. Currently human interaction is necessary for processing the results. By locating the highest mutation scores returned and analyzing the SQL relating to that score the bug can be pinpointed. If this process could be reliably replicated through an extension of the program it could be made significantly more effective and reduce the time spent pinpointing a specific problem.

\bibliographystyle{plain}
\bibliography{paper_review_lab9_cs580Spring2013}

\end{document}
